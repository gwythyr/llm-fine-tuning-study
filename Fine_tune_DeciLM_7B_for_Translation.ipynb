{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gwythyr/llm-fine-tuning-study/blob/master/Fine_tune_DeciLM_7B_for_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRHbfszIfsoi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "os.environ['LANG'] = 'en_US.UTF-8'\n",
        "os.environ['LC_CTYPE'] = 'en_US.UTF-8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GvNurZjfuM6"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBDAuQPpfylh"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "def create_directory(path: Optional[Path] = None, dir_name: str = \"output\"):\n",
        "    \"\"\"\n",
        "    Creates a directory at the specified path with the given directory name.\n",
        "    If no path is provided, the current working directory is used.\n",
        "\n",
        "    Parameters:\n",
        "    - path (Optional[Path]): The path where the directory is to be created.\n",
        "    - dir_name (str): The name of the directory to create.\n",
        "\n",
        "    Returns:\n",
        "    - Path object representing the path to the created directory.\n",
        "    \"\"\"\n",
        "    # Use the current working directory if no path is provided\n",
        "    working_dir = path if path is not None else Path('./')\n",
        "\n",
        "    # Define the output directory path by joining paths\n",
        "    output_directory = working_dir / dir_name\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    output_directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    return output_directory\n",
        "\n",
        "output_dir = create_directory(dir_name=\"fine-tuned-checkpoints\")\n",
        "print(f\"Directory created at: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXBCHF9bepux"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U trl\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6DoSEUhq0U-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZoomtwMgQn6"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7LDKQ7OgNm1"
      },
      "outputs": [],
      "source": [
        "model_name = \"Deci/DeciLM-7B\"\n",
        "\n",
        "gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "\n",
        "do_quantization = gpu_memory < 20e9\n",
        "\n",
        "if do_quantization:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit = True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        # bfloat works only on A100 (or ampere supported chip)\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        # if you're using a T4 or non-ampere chip comment out the above and run this instead:\n",
        "        # bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    decilm = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=True,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "else:\n",
        "    decilm = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=True,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLZssWffiyIr"
      },
      "source": [
        "# Load dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg9DRDL0jv7e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = \"harpreetsahota/modern-to-shakesperean-translation\"\n",
        "\n",
        "data = load_dataset(dataset, split=\"train\")\n",
        "\n",
        "data = data.shuffle(seed=42)\n",
        "\n",
        "modern_to_shakespearean = data.train_test_split(test_size=0.1, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_short_translation_prompt(sample):\n",
        "  prompt = \"<s>\"\n",
        "  prompt += sample[\"modern\"]\n",
        "  prompt += \" ###> \"\n",
        "  prompt += sample[\"shakespearean\"]\n",
        "  prompt += \"</s>\"\n",
        "  return {\"text\" : prompt}"
      ],
      "metadata": {
        "id": "A2dtwC4jtIy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can try this prompt which has more of an instruction tune feel to it."
      ],
      "metadata": {
        "id": "Ux0MpPrizHoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2Ce7A5hlXOe"
      },
      "outputs": [],
      "source": [
        "# def construct_translation_prompt(sample):\n",
        "#   prompt = \"\"\n",
        "#   prompt += \"Translate the following text from Modern English to Shakespearean English\"\n",
        "#   prompt += \"\\n\\n### Modern English: \\n\"\n",
        "#   prompt += sample[\"modern\"]\n",
        "#   prompt += \"\\n\\n### Shakespearean English: \\n\"\n",
        "#   prompt += sample[\"shakespearean\"]\n",
        "#   return {\"text\" : prompt}\n",
        "\n",
        "# modern_to_shakespearean = modern_to_shakespearean.map(construct_translation_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modern_to_shakespearean = modern_to_shakespearean.map(construct_short_translation_prompt)"
      ],
      "metadata": {
        "id": "bohnZ72suVzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SbU606Utz-6"
      },
      "outputs": [],
      "source": [
        "modern_to_shakespearean['train'][42]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feFd7Izejv_C"
      },
      "source": [
        "# QLoRA Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgtdY3TJjwCo"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "# we set our lora config to be the same as qlora\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    #  The modules to apply the LoRA update matrices.\n",
        "    target_modules = [\"gate_proj\", \"down_proj\", \"up_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3HLCtgMjwGH"
      },
      "source": [
        "# Prepare model for peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbEzOJLXjwJO"
      },
      "outputs": [],
      "source": [
        "if do_quantization:\n",
        "    decilm = prepare_model_for_kbit_training(decilm)\n",
        "\n",
        "decilm.enable_input_require_grads()\n",
        "decilm = get_peft_model(decilm, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzopDb-CjwMV"
      },
      "source": [
        "# Training Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMTW7cYlklqq"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        do_eval=True,\n",
        "        auto_find_batch_size=True,\n",
        "        log_level=\"debug\",\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        save_steps=25,\n",
        "        logging_steps=100,\n",
        "        learning_rate=3e-4,\n",
        "        weight_decay=0.01,\n",
        "        # basically just train for 5 epochs, you should train for longer\n",
        "        max_steps=len(modern_to_shakespearean['train']) * 5,\n",
        "        warmup_steps=150,\n",
        "        # if you're using a T4, or non-ampere supported chip comment out the below line.\n",
        "        bf16=True,\n",
        "        tf32=True,\n",
        "        gradient_checkpointing=True,\n",
        "        max_grad_norm=0.3, #from the paper\n",
        "        lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmCRY8-Jk8vY"
      },
      "source": [
        "# Train\n",
        "\n",
        "Super short training run, takes ~15 minutes on an A100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNTDQ9j4k_PS"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=decilm,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field='text',\n",
        "    train_dataset=modern_to_shakespearean['train'],\n",
        "    eval_dataset=modern_to_shakespearean['test'],\n",
        "    max_seq_length=4096,\n",
        "    dataset_num_proc=os.cpu_count(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48rOgUt0k_UT"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqIqoortk_Ys"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_meh8NTlFAK"
      },
      "source": [
        "# Merge adapter to base model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_a7-9OglFEc"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from functools import partial\n",
        "\n",
        "AutoTokenizer.from_pretrained = partial(AutoTokenizer.from_pretrained, trust_remote_code=True)\n",
        "\n",
        "instruction_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    training_args.output_dir,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map = 'auto',\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "merged_model = instruction_tuned_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0icyG9otlFJA"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"max_new_tokens\": 32,\n",
        "    \"early_stopping\": True,\n",
        "    \"num_beams\": 5,\n",
        "    \"temperature\" : 0.001,\n",
        "    \"do_sample\":True,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"repetition_penalty\" : 1.5,\n",
        "    \"renormalize_logits\": True\n",
        "}\n",
        "\n",
        "decilm_tuned_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=merged_model,\n",
        "    tokenizer=tokenizer,\n",
        "    **generation_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_9j7DaV0Gew"
      },
      "outputs": [],
      "source": [
        "modern_sayings = [\n",
        "    \"Bruh, Stop cappin', I know you ain't about that life homie.\",\n",
        "    \"Stop throwing shade at me, Fam. You know I keep it 100 with you.\",\n",
        "    \"I'm gonna dip homie, these folks are sus. Catch you later.\",\n",
        "    \"Yo fam, he slid into my DMs and now he's simping all over me.\",\n",
        "    \"I'm lowkey obsessed with this song, it's such a mood.\",\n",
        "    \"He's big mad 'cause he took an L in the game.\",\n",
        "    \"She really feeling herself after that glow up.\",\n",
        "    \"Yo homie, why you trippin' over her like that being a simp?\",\n",
        "    \"No cap bruh, you lookin hella chuegy in them skinny jeans.\"\n",
        "]\n",
        "\n",
        "def construct_inference_prompt(input_text):\n",
        "    prompt = \"\"\n",
        "    # prompt += \"Translate the following text from Modern English to Shakespearean English\"\n",
        "    # prompt += \"\\n\\n### Modern English: \\n\"\n",
        "    prompt += input_text\n",
        "    prompt += \" ###>\"\n",
        "    # prompt += \"\\n\\n### Shakespearean English: \\n\"\n",
        "    return prompt\n",
        "\n",
        "def translate_modern_to_shakespearean(input_phrase):\n",
        "    modern_saying = construct_inference_prompt(input_phrase)\n",
        "    translation_result = decilm_tuned_pipeline(modern_saying, return_full_text=True)[0]['generated_text']\n",
        "    print(translation_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_modern_to_shakespearean(modern_sayings[7])"
      ],
      "metadata": {
        "id": "UAVAisMhK8Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_modern_to_shakespearean(modern_sayings[2])"
      ],
      "metadata": {
        "id": "vAw17nmVLSGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}